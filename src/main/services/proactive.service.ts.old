import Anthropic from '@anthropic-ai/sdk'
import { EventEmitter } from 'events'
import { loadSettings } from '../config/settings.config'

/**
 * Configuration for memu API
 */
const MEMU_CONFIG = {
  baseUrl: 'https://api.memu.so',
  apiKey: 'mu_Ow-vZFGcNbmCmptZ937XbEzR14PWjFuVbFIxku7oCyCrkn7ri1bRfa3_7y8ieddJ65IIfe6odylLtAPNOIiOVH6LvMXKV6G5LnVOcw',
  userId: 'bot_proactive_user',
  agentId: 'bot_proactive_agent'
}

/**
 * Number of messages before triggering memorization
 */
const N_MESSAGES_MEMORIZE = 2

/**
 * Maximum context messages to restore
 */
const MAX_CONTEXT_MESSAGES = 20

/**
 * Streaming message types emitted during autorun
 */
export type StreamMessageType = 
  | 'user_message'      // User or auto-generated input
  | 'assistant_chunk'   // Streaming text chunk from assistant
  | 'assistant_done'    // Assistant response complete
  | 'tool_start'        // Tool execution starting
  | 'tool_result'       // Tool execution result
  | 'status_change'     // Status changed (thinking, idle, etc.)
  | 'memorize_trigger'  // Memorization triggered
  | 'error'             // Error occurred

export interface StreamMessage {
  type: StreamMessageType
  data: unknown
  timestamp: number
}

/**
 * Conversation message for storage
 */
export interface StoredMessage {
  role: 'user' | 'assistant'
  content: string
  timestamp: number
  isAutoGenerated?: boolean
}

/**
 * Autorun status
 */
export type AutorunStatus = 'idle' | 'thinking' | 'tool_executing' | 'waiting_input'

/**
 * Memu tools definitions for direct use with Anthropic API
 */
const memuTools: Anthropic.Tool[] = [
  {
    name: 'memu_memory',
    description: 'Retrieve memory based on a query. Use this to recall past conversations, facts, or context about the user.',
    input_schema: {
      type: 'object',
      properties: {
        query: {
          type: 'string',
          description: 'The query to search memory for'
        }
      },
      required: ['query']
    }
  },
  {
    name: 'memu_todos',
    description: 'Retrieve todos for the user. Returns a list of pending tasks and their summaries.',
    input_schema: {
      type: 'object',
      properties: {},
      required: []
    }
  }
]

/**
 * System prompt for autorun agent
 */
const AUTORUN_SYSTEM_PROMPT = `You are a helpful AI assistant working in an autonomous mode. You help the user accomplish tasks and manage their todos.

You have access to:
1. **memu_memory** - Retrieve relevant memories and context from past conversations
2. **memu_todos** - Get the user's current todo list

Guidelines:
- When given todos, help the user make progress on them
- Use memory retrieval to provide personalized, context-aware responses
- Be proactive in suggesting next steps
- Keep responses concise and actionable
- If you complete a todo item, mention it explicitly`

/**
 * Simple in-memory storage for conversation messages
 * In production, replace with persistent storage (database, file, etc.)
 */
class ConversationStorage {
  private messages: StoredMessage[] = []
  private storageKey = 'autorun_conversation'

  async saveMessage(message: StoredMessage): Promise<void> {
    this.messages.push(message)
    // In production: persist to database/file
  }

  async getMessages(limit: number = MAX_CONTEXT_MESSAGES): Promise<StoredMessage[]> {
    // Return most recent messages up to limit
    return this.messages.slice(-limit)
  }

  async clearMessages(): Promise<void> {
    this.messages = []
  }

  async getMessageCount(): Promise<number> {
    return this.messages.length
  }
}

/**
 * Execute memu_memory tool
 */
async function executeMemuMemory(query: string): Promise<{ success: boolean; data?: unknown; error?: string }> {
  try {
    const response = await fetch(`${MEMU_CONFIG.baseUrl}/api/v3/memory/retrieve`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${MEMU_CONFIG.apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        user_id: MEMU_CONFIG.userId,
        agent_id: MEMU_CONFIG.agentId,
        query
      })
    })
    const result = await response.json()
    return { success: true, data: result }
  } catch (error) {
    return { success: false, error: error instanceof Error ? error.message : String(error) }
  }
}

/**
 * Execute memu_todos tool
 */
async function executeMemuTodos(): Promise<{ success: boolean; data?: unknown; error?: string }> {
  try {
    const response = await fetch(`${MEMU_CONFIG.baseUrl}/api/v3/memory/categories`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${MEMU_CONFIG.apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        user_id: MEMU_CONFIG.userId,
        agent_id: MEMU_CONFIG.agentId
      })
    })
    const result = await response.json() as { categories: Array<{ name: string; summary: string }> }
    
    // Extract todos from categories
    let todos = ''
    for (const category of result.categories || []) {
      if (category.name === 'todo') {
        todos = category.summary
        break
      }
    }
    
    return { success: true, data: { todos } }
  } catch (error) {
    return { success: false, error: error instanceof Error ? error.message : String(error) }
  }
}

/**
 * Get todos directly (for checking if autorun should continue)
 */
async function getTodos(): Promise<string> {
  const result = await executeMemuTodos()
  if (result.success && result.data) {
    return (result.data as { todos: string }).todos || ''
  }
  return ''
}

/**
 * Create Anthropic client with current settings
 */
async function createClient(): Promise<{ client: Anthropic; model: string; maxTokens: number }> {
  const settings = await loadSettings()

  if (!settings.claudeApiKey) {
    throw new Error('Claude API key not configured. Please set it in Settings.')
  }

  const client = new Anthropic({
    apiKey: settings.claudeApiKey
  })

  return {
    client,
    model: settings.claudeModel || 'claude-sonnet-4-20250514',
    maxTokens: settings.maxTokens || 4096
  }
}

/**
 * Memorize conversation messages
 */
async function triggerMemorize(messages: StoredMessage[]): Promise<boolean> {
  try {
    const formattedMessages = messages.map(m => ({
      role: m.role,
      content: m.content
    }))
    
    const response = await fetch(`${MEMU_CONFIG.baseUrl}/api/v3/memory/memorize`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${MEMU_CONFIG.apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        user_id: MEMU_CONFIG.userId,
        agent_id: MEMU_CONFIG.agentId,
        messages: formattedMessages
      })
    })
    
    return response.ok
  } catch (error) {
    console.error('[Autorun] Memorization failed:', error)
    return false
  }
}

/**
 * AutorunService handles autonomous conversation with Claude
 * Supports both user-triggered and auto-triggered (via todos) message processing
 * Uses streaming for real-time response delivery
 */
export class AutorunService extends EventEmitter {
  private conversationHistory: Anthropic.MessageParam[] = []
  private storage: ConversationStorage
  private currentStatus: AutorunStatus = 'idle'
  private isAborted = false
  private contextLoaded = false
  private roundCount = 0

  constructor() {
    super()
    this.storage = new ConversationStorage()
  }

  /**
   * Get current status
   */
  getStatus(): AutorunStatus {
    return this.currentStatus
  }

  /**
   * Set and emit status change
   */
  private setStatus(status: AutorunStatus): void {
    this.currentStatus = status
    this.emitStream('status_change', { status })
  }

  /**
   * Emit a stream message
   */
  private emitStream(type: StreamMessageType, data: unknown): void {
    const message: StreamMessage = {
      type,
      data,
      timestamp: Date.now()
    }
    this.emit('stream', message)
  }

  /**
   * Abort current processing
   */
  abort(): void {
    console.log('[Autorun] Aborting...')
    this.isAborted = true
    this.setStatus('idle')
  }

  /**
   * Check if currently processing
   */
  isProcessing(): boolean {
    return this.currentStatus !== 'idle' && this.currentStatus !== 'waiting_input'
  }

  /**
   * Load context from storage
   */
  private async loadContext(): Promise<void> {
    if (this.contextLoaded) return

    console.log('[Autorun] Loading historical context...')
    const messages = await this.storage.getMessages()
    
    if (messages.length > 0) {
      let lastRole: 'user' | 'assistant' | null = null
      
      for (const msg of messages) {
        if (!msg.content) continue
        
        // Anthropic API requires alternating user/assistant messages
        if (msg.role === lastRole && this.conversationHistory.length > 0) {
          const lastMsg = this.conversationHistory[this.conversationHistory.length - 1]
          if (typeof lastMsg.content === 'string') {
            lastMsg.content = lastMsg.content + '\n\n' + msg.content
          }
        } else {
          this.conversationHistory.push({
            role: msg.role,
            content: msg.content
          })
          lastRole = msg.role
        }
      }
      
      console.log(`[Autorun] Loaded ${this.conversationHistory.length} context messages`)
    }
    
    this.contextLoaded = true
  }

  /**
   * Process a user message (event-driven, replaces the while True loop for user input)
   * Returns an async generator for streaming responses
   */
  async *processMessage(userMessage: string): AsyncGenerator<StreamMessage> {
    await this.loadContext()
    
    this.isAborted = false
    this.setStatus('thinking')
    
    // Emit user message
    const userStreamMsg: StreamMessage = {
      type: 'user_message',
      data: { content: userMessage, isAutoGenerated: false },
      timestamp: Date.now()
    }
    yield userStreamMsg
    
    // Store user message
    await this.storage.saveMessage({
      role: 'user',
      content: userMessage,
      timestamp: Date.now(),
      isAutoGenerated: false
    })
    
    // Add to conversation history
    this.conversationHistory.push({
      role: 'user',
      content: userMessage
    })
    
    // Run agent loop with streaming
    for await (const msg of this.runStreamingAgentLoop()) {
      yield msg
    }
    
    this.roundCount++
    
    // Check for memorization
    const messageCount = await this.storage.getMessageCount()
    if (messageCount >= N_MESSAGES_MEMORIZE) {
      yield {
        type: 'memorize_trigger',
        data: { messageCount },
        timestamp: Date.now()
      }
      
      const messages = await this.storage.getMessages()
      const success = await triggerMemorize(messages)
      if (success) {
        await this.storage.clearMessages()
      }
    }
    
    this.setStatus('idle')
  }

  /**
   * Check if there are pending todos and run automatically if so
   * Returns an async generator for streaming all messages (both auto-generated input and responses)
   * This is the "long connection" streaming approach
   */
  async *runAutoLoop(): AsyncGenerator<StreamMessage> {
    await this.loadContext()
    
    // Only auto-run if we have had at least one user interaction
    if (this.roundCount === 0) {
      this.setStatus('waiting_input')
      return
    }
    
    // Check for todos
    const todos = await getTodos()
    
    if (!todos) {
      this.setStatus('waiting_input')
      return
    }
    
    // We have todos - generate auto message and process
    this.isAborted = false
    this.setStatus('thinking')
    
    const autoMessage = `Please continue with the following todos: ${todos}`
    
    // Emit auto-generated user message
    yield {
      type: 'user_message',
      data: { content: autoMessage, isAutoGenerated: true },
      timestamp: Date.now()
    }
    
    // Store auto message
    await this.storage.saveMessage({
      role: 'user',
      content: autoMessage,
      timestamp: Date.now(),
      isAutoGenerated: true
    })
    
    // Add to conversation history
    this.conversationHistory.push({
      role: 'user',
      content: autoMessage
    })
    
    // Run agent loop with streaming
    for await (const msg of this.runStreamingAgentLoop()) {
      yield msg
    }
    
    this.roundCount++
    
    // Check for memorization
    const messageCount = await this.storage.getMessageCount()
    if (messageCount >= N_MESSAGES_MEMORIZE) {
      yield {
        type: 'memorize_trigger',
        data: { messageCount },
        timestamp: Date.now()
      }
      
      const messages = await this.storage.getMessages()
      const success = await triggerMemorize(messages)
      if (success) {
        await this.storage.clearMessages()
      }
    }
    
    this.setStatus('idle')
  }

  /**
   * Run the streaming agent loop
   * Uses Anthropic streaming API for real-time response delivery
   */
  private async *runStreamingAgentLoop(): AsyncGenerator<StreamMessage> {
    // Create client with current settings (re-read each time in case settings changed)
    let client: Anthropic
    let model: string
    let maxTokens: number

    try {
      const clientConfig = await createClient()
      client = clientConfig.client
      model = clientConfig.model
      maxTokens = clientConfig.maxTokens
    } catch (error) {
      yield {
        type: 'error',
        data: { message: error instanceof Error ? error.message : 'Failed to create client' },
        timestamp: Date.now()
      }
      return
    }
    
    let iterations = 0
    const maxIterations = 20
    
    while (iterations < maxIterations) {
      if (this.isAborted) {
        yield {
          type: 'error',
          data: { message: 'Aborted by user' },
          timestamp: Date.now()
        }
        return
      }
      
      iterations++
      console.log(`[Autorun] Iteration ${iterations}`)
      this.setStatus('thinking')
      
      // Use streaming API for real-time response
      const stream = await client.messages.stream({
        model,
        max_tokens: maxTokens,
        system: AUTORUN_SYSTEM_PROMPT,
        tools: memuTools,
        messages: this.conversationHistory
      })
      
      let fullText = ''
      let toolUseBlocks: Anthropic.ToolUseBlock[] = []
      
      // Stream text chunks
      for await (const event of stream) {
        if (this.isAborted) break
        
        if (event.type === 'content_block_delta') {
          const delta = event.delta as { type: string; text?: string }
          if (delta.type === 'text_delta' && delta.text) {
            fullText += delta.text
            yield {
              type: 'assistant_chunk',
              data: { text: delta.text, fullText },
              timestamp: Date.now()
            }
          }
        }
      }
      
      // Get final message
      const finalMessage = await stream.finalMessage()
      
      // Collect tool use blocks
      toolUseBlocks = finalMessage.content.filter(
        (block): block is Anthropic.ToolUseBlock => block.type === 'tool_use'
      )
      
      // If we have tool use, process tools
      if (finalMessage.stop_reason === 'tool_use' && toolUseBlocks.length > 0) {
        // Add assistant response to history
        this.conversationHistory.push({
          role: 'assistant',
          content: finalMessage.content
        })
        
        // Execute tools
        const toolResults: Anthropic.ToolResultBlockParam[] = []
        
        for (const toolUse of toolUseBlocks) {
          if (this.isAborted) break
          
          this.setStatus('tool_executing')
          
          yield {
            type: 'tool_start',
            data: { name: toolUse.name, input: toolUse.input },
            timestamp: Date.now()
          }
          
          const result = await this.executeTool(toolUse.name, toolUse.input)
          
          yield {
            type: 'tool_result',
            data: { name: toolUse.name, result },
            timestamp: Date.now()
          }
          
          toolResults.push({
            type: 'tool_result',
            tool_use_id: toolUse.id,
            content: JSON.stringify(result),
            is_error: !result.success
          })
        }
        
        // Add tool results to history
        this.conversationHistory.push({
          role: 'user',
          content: toolResults
        })
        
        // Continue loop to get response after tool use
        continue
      }
      
      // No tool use - we have a final response
      if (fullText) {
        // Store assistant message
        await this.storage.saveMessage({
          role: 'assistant',
          content: fullText,
          timestamp: Date.now()
        })
        
        // Add to history
        this.conversationHistory.push({
          role: 'assistant',
          content: finalMessage.content
        })
        
        yield {
          type: 'assistant_done',
          data: { text: fullText },
          timestamp: Date.now()
        }
      }
      
      // Done
      return
    }
    
    yield {
      type: 'error',
      data: { message: 'Max iterations reached' },
      timestamp: Date.now()
    }
  }

  /**
   * Execute a memu tool directly
   */
  private async executeTool(
    name: string,
    input: unknown
  ): Promise<{ success: boolean; data?: unknown; error?: string }> {
    console.log(`[Autorun] Executing tool: ${name}`)
    
    switch (name) {
      case 'memu_memory': {
        const args = input as { query: string }
        return await executeMemuMemory(args.query)
      }
      
      case 'memu_todos': {
        return await executeMemuTodos()
      }
      
      default:
        return { success: false, error: `Unknown tool: ${name}` }
    }
  }

  /**
   * Clear conversation history and storage
   */
  async clearHistory(): Promise<void> {
    this.conversationHistory = []
    await this.storage.clearMessages()
    this.contextLoaded = false
    this.roundCount = 0
  }

  /**
   * Get current round count
   */
  getRoundCount(): number {
    return this.roundCount
  }
}

// Export singleton instance
export const autorunService = new AutorunService()

